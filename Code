# import relevant libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# OPTIONAL:
# import pearson correlation library
from scipy.stats import pearsonr

# to show grid lines in plots
sns.set_style('whitegrid')

# to make all plots well positioned in the notebook
%matplotlib inline


# Import the Boston DataSet
from sklearn.datasets import load_boston


# store the dataset
boston = load_boston()


# check the different keys
boston.keys()


# You can choose to print each of these keys to see their content
# kindly delete the '#' and run the code to see

## for data
# print(boston['data'])

## for target or predictors
# print(boston['target'])

## for feature_names OR predictor/column names
# print(boston['feature_names'])

## for DESCR: description of each feature/predictor
print(boston['DESCR'])

## for filename 
# (not relevant)



## Exploratory Data Analysis
#### Let's explore the data a little !


# chect the current data type
type(boston['data'])


# Convert the numpy array " boston['data'] " into a dataframe
bostonData_array = boston['data']

boston_df = pd.DataFrame(bostonData_array, columns = ['CRIM','ZN','INDUS','CHAS', 'NOX', 'RM',
                                                      'AGE', 'DIS', 'RAD', 'TAX', ' PTRATIO', 'B', 'LSTAT'])

# add the RESPONSE Variable
boston_df['Med. Worth of Home'] = boston['target']

boston_df.head()


###### (1) Compare 
# the 'Average number of rooms per dwelling [RM]' (predictor) with the 'Median value of owner-occupied homes in $1000's [boston['target']] (the response vairable). 

######  Does the correlation make sense?

sns.jointplot(x = 'RM',
              y = 'Med. Worth of Home',
              data = boston_df ,
              color = 'k',
              stat_func = pearsonr)   # optional for the correlation value and p-value
              

###### YES, correlation makes sense.
# There exist is a positive correlation/relationship between  RM (Average number of rooms per dwelling) and boston['target'] (Median value of owner-occupied homes in $1000's). 
# This could possibly infer that, higher average number of rooms per dwelling would result to a higher median value of the homes.


##### (2) Let us do a pair plot to see the relationship between "selected" predictors/columns and their correlation
sns.pairplot(data = boston_df[
                                ['CRIM', 'RM', 'AGE', 'DIS', 'TAX', 'Med. Worth of Home']
                             ]
            )

### We can as well create a linear model plot
sns.lmplot(x = 'RM', y = 'Med. Worth of Home', data = boston_df) 


## It will be nice to represent this 'RM' and 'Med. Worth of Home' with a hex plot
sns.jointplot(x = 'RM',
              y = 'Med. Worth of Home',
              data = boston_df ,
              kind = 'hex',          # also try 'scatter', 'reg', 'resid', 'kde' *it is optional
              color = 'k',
              stat_func = pearsonr)


#### Interpretation:
    ** There is a dense between 5.5 to 7.0 in 'RM' which corresponds to between 15.0 to 25.0 in the Median Worth of Home,
    ** This implies that most owner-occupied homes in Boston have an average of 6 rooms per dwelling and the median worth of these homes is around between 150,000 to 250,000
    ** We see that there is a strong correlation between the average number of rooms per dwelling and the Median Value or Worth of the Homes. Correlation is strong being 0.7.





## Training and Testing Data

# Now that we've explored the data a bit, let's go ahead and split the data into training and testing sets.

# ** Our variable X will equal the numerical features/columns which is boston['data'] ** 

# ** Our variable y will equal the response variable which is boston['target'], i.e. Median value of owner-occupied homes in \$1000's **


# predictors
X = boston['data']


# response (predicands)
y = boston['target']


# Split data into training and testing set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size = 0.3, # 30% of data should be used to test so # 70% to train
                                                    random_state = 101)



#### Train the Model with train-data

# import LinearRegression
from sklearn.linear_model import LinearRegression


# create an object for the linear regression
lm = LinearRegression()


# train and fit 'lm' on the training data
lm.fit(X = X_train,
       y = y_train)


# Get the coefficients of the predictors

lm.coef_


# Get the intercept of the predictors

lm.intercept_


## The model is thus given by:

print("Our linear model is: "
      " 'Medain Value of Home (Y)' = {:.4} + {:.4}*CRIM + {:.4}*ZN + {:.4}*INDUS + {:.4}*CHAS + {:.4}*NOX + {:.4}*RM + "
                                           " {:.4}*AGE + {:.4}*DIS + {:.4}*RAD + {:.4}*TAX + {:.4}*PTRATIO + {:.4}*B + "
                                           " {:.4}*LSTAT ".format(
                                   lm.intercept_, 
                                lm.coef_[0], lm.coef_[1], lm.coef_[2], lm.coef_[3], lm.coef_[4], lm.coef_[5], 
                           lm.coef_[6], lm.coef_[7], lm.coef_[8], lm.coef_[9], lm.coef_[10], lm.coef_[11], lm.coef_[12]))



### Prediction of Model
** We evaluate its performance of our model by predicting the test values! **

predictions = lm.predict(X = X_test)

# see the values
predictions



### Compare:
**Now let's see how strong the relationship is, between our predictions and the real (original y-values)**


# Using a scatter plot to check for correlation

# Approach 1
# Using matplotlib

plt.scatter(x = y_test,
            y = predictions)


# Approach 2
# Using seaborn scatterplot

#sns.scatterplot(x = y_test,        # original or real values from data
#                y = predictions)   # predicted values


# Approach 3
# Using Seaborn Jointplot (so we can call the correlation value)

sns.jointplot(x = y_test,
              y = predictions,
              kind = 'scatter',
              stat_func = pearsonr)

### Interpreation:
    ** There is obviously a strong correlation between our prediction and the original values. Correlation value is 0.85,
    hence our model is good enough to be used for predictions in real life.


## Evaluating the Model
** Let's evaluate our model performance by calculating the residual sum of squares and the variance score (R^2) **

# import the library

from sklearn import metrics

##### Quick important note:
   ** The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close 
   the observed data points are to the model’s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. 
   As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being 
   in the same units as the response variable. **Lower values of RMSE indicate better fit**. RMSE is a good measure of how accurately the model predicts the response, 
   and it is the most important criterion for fit if the main purpose of the model is prediction. 
   (Source: https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/)**


# Since the RMSE is low, we can say that our model accurately predicts the reponse.



## Residuals:
    ** Let's quickly explore the residuals to make sure everything was okay with our data. We do this by ploting the
    histogram of the residuals to ensure it is normmally distributed **


# Using Seaborn

sns.distplot(a = (y_test - predictions),   # this is how residual is calculated
             bins = 50)
             
# YES: there is a good level of normality in our residuals. Hence, we finally accepts the model


## Conclusion:
#### Significance of predictors
    ** We can tell which predictor is more powerful (i.e. more significant) in predicting the response variable
    ** We do this in two methods:
        *** Using the coefficients of the predictors
        *** Using the p-values of the predictors
        



##### Method 1 (Coefficient of predictors)
# Create a dataframe that will take the coefficeint and also the column names


# recall the column names
boston_df.columns


# remove the last column (i.e. the predictor)

boston_df.drop(labels = 'Med. Worth of Home',   # name of the column to drop
               axis = 1,      # means column, axis = 0 means row 
               inplace = True)    # make the drop permanent

# now check the columns again
boston_df.columns


# DataFrame

cdf = pd.DataFrame(data = lm.coef_,
                   index = boston_df.columns,
                   columns = ['Coefficient'])

cdf


# Let's sort the values of by the Coefficient column

cdf.sort_values(by = ['Coefficient'],
                ascending = False)





### Interpretation 1:
    ** We see the significance of each predictor (based on their coefficients) in the above table in descendeing order
    ** Clearly, 'CHAS'':Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)' and 'RM: ''average number of rooms per dwelling', 
       happens to be the most significnat
    ** while 'NOX: '' nitric oxides concentration (parts per 10 million)' is the least significant


##### Method 2 (Using p-values)
# This method is like a short cut of everything we have been doing
# It output all the above we result we have done so far,
# the confidence interval, p-values at 95% confidence level,
# AIC, BIC, and more


import statsmodels.api as sm

X = X   # columns or predictors 
y = y  # predictor == Price 

X2 = sm.add_constant(X)
est = sm.OLS(y, X2)   # OLS means Ordinary Least Squares
est2 = est.fit()
print(est2.summary() )



### Interpretation 2:
    ** We see the significance of each predictor (based on their coefficients) in the above table. We see that the p-values (P>|t|) 
       are all less than 0.05 for all predictors except for two, 2 variables
    ** The variables 'x1' to 'x13' represents the same order with which the columns are described in boston['DESCR']
    ** Clearly, all variables are siginificant at a 95% confidence level except 'x3' and 'x7'. They both respectively corresponds to:
        *** ' INDUS:    proportion of non-retail business acres per town ' and 
        *** 'AGE:    proportion of owner-occupied units built prior to 1940'


# THANK YOU!
# OKIS









